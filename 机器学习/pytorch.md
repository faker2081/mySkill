# pytorch

**zip()压缩可迭代对象，\*号解压可迭代对象**

### 什么是Pytorch

Pytorch是一个流行的深度学习框架，由Facebook人工智能研究（FAIR）开发和维护，用于处理张力。自2016年1.0.0发布以来，由于使用的简单性和灵活性，它获得了极大的普及。在本文中，我们将主要关注使用Pytorch的一些核心张量运算。您可能想通过这个博客了解如何安装PyTorch的详细说明。

1. torch.size
2. torch.mm
3. torch.cat
4. torch.mul
5. torch.inverse

#### 1.torch.size

Torch.size返回任何输入张量的维度。

在上面的示例中，我创建了一个3X2X4张量，torch.size返回3个维度。 我们的张量在外括号内有3个元素，每个元素都是一个矩阵。 每个矩阵同样有2个元素，每个元素都是一个包含4个元素的列表。

#### 2.torch.mm

torch.mm返回任意两个输入矩阵matr1和mat2的矩阵乘法(不是元素一一对应相乘)

在上面的示例中，mat1和mat2的大小均为3X3。 因此，torch.mm的输出大小也为3X3。 我们可以看到可以使用“ @”运算符代替torch.mm来执行相同的操作。

使用torch.mm()时要注意的几点

- 第一个输入矩阵的列大小应等于第二个输入矩阵的行大小
- 对于不是矩阵或大于2维的张量，torch.mm不起作用，我们可以使用torch.mul进行逐个元素的乘法
- ‘@'运算符执行与torch.mm相同的操作

#### 3.torch.cat

Torch.cat可以水平或垂直连接2个张量

我们可以看到，张量y已经堆叠在张量x的下方。

通过使用dim参数并将其设置为1，我们可以水平连接2个张量。 默认设置为0，这将导致垂直串联。

使用torch.cat时的注意事项

- 使用dim = 1来水平连接张量
- 可以连接任意数量的张量，但是，请确保张量在串联方向上的大小应相同

#### 4.torch.mul

torch.mul 可对2个张量进行逐元素乘法。

我们可以看到逐元素乘法适用于任何维度的张量。使用*运算符或者使用a.mul(b)，其中a和b为输入张量，也可以实现torch.mul的功能。

使用torch.mul时要注意的几点

- torch.mul类似于两个向量之间的点积。
- “ *”运算符或a.mul（b）也执行与torch.mm相同的操作
- 输入张量应满足广播条件

#### 5.torch.inverse

torch.inverse计算任意张量的逆

这里，我们通过使用randn函数来建立了一个由随机数组成的4x4张量。torch.inverse可计算x的逆。Inv(x) @ X 则会返回一个单位矩阵。

使用torch.inverse需要注意的几点：

- 只有在输入张量的各个维度的元素数量相同时才可以正常使用torch.inverse
- torch.inverse 同样适用于3维张量。但是张量在各个方向上的维度需要相同或者为方阵张量。

### 权重衰退

- 一般来说不限制b(偏移)
- 小的$\theta$ 以为更强的正则项（当$\lambda$  趋于 $\inf$ 的时候）
- 罚函数：柔性限制  

###### 用L2范数，解释

事实上，这些选择（L1、L2）在整个统计领域中都是有效的和受欢迎的。L2正则化线性模型构成经典的*岭回归*（ridge regression）算法，L1正则化线性回归是统计学中类似的基本模型，通常被称为*套索回归*（lasso regression）。

### 丢弃法（DropOut）

- 丢弃发将一些输出项随机置0来控制模型复杂度
- 常作用在多层感知机的隐藏输出上
- 丢弃概率是控制模型复杂度的超参数

### 数值稳定

防止梯度消失和梯度爆炸

