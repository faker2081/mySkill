

## 建模步骤

<img src="C:\Users\20844\AppData\Roaming\Typora\typora-user-images\image-20210321175601207.png" alt="image-20210321175601207" style="zoom:200%;" />

![image-20210321175748417](C:\Users\20844\AppData\Roaming\Typora\typora-user-images\image-20210321175748417.png)

![image-20210321212132684](C:\Users\20844\AppData\Roaming\Typora\typora-user-images\image-20210321212132684.png)

*som网络聚类、高斯混合聚类*

1. 模型准备
2. 模型假设
3. 模型建立
4. 模型求解
5. 模型分析
6. 模型检验

### 赛题类型

1. 数据处理

   - 插值拟合
     主要用于对数据的补全和基本的趋势分析

   - 小波分析，聚类分析（高斯混合聚类，K-均值聚类等等)
     主要用于诊断数据异常值并进行剔除

   - 主成分分析、线性判别分析、局部保留投影等
     主要用于多维数据的降维处理，减少数据冗余

   - 均值、方差分析、协方差分析等统计方法

     主要用于数据的街区或者特征值选择

2. 关联与因果分析

   - Superman或kendall等级相关分析
   - Person相关（样本点的个数比较多）
   - Copula相关（比较难，金融数学，概率密度）
   - 典型相关分析（因变量Y1234,自变量组X1234，各自变量组相关
     性比较强，问哪一个因变量与哪一个自变量关系比较紧密?)
   - 灰色关联分析方法（样本点的个数较少

3. 分类与判别

   - ①距离聚类（系统聚类）常用
   - ②关联性聚类（常用）
   - ③层次聚类
   - ④密度聚类
   - ⑤其他聚类
   - ⑥贝叶斯判别（统计判别方法)
   - ⑦费舍尔判别（训练的样本比较少)
   - ⑧模糊识别（分好类的数据点比较少)

4. 评价与决策

   - 模糊综合评判:评价一个对象优、良、中、差等层次评价，评价一个学校等，不能排序
   - 主成分分析:评价多个对象的水平并排序，指标间关联性很强。
   - 层次分析法:做决策，通过指标，综合考虑做决定（低级）
   - 数据包络(DEA)分析法:优化问题，对各省发展状况进行评判
   - 秩和比综合评价法:评价各个对象并排序，指标间关联性不强
   - 神经网络评价:适用于多指标非线性关系明确的评价（常用）

5. 预测与预报

   主要有五种:
   小样本内部预测
   大样本的内部预测
   小样本的未来预测
   大样本的随机因素或周期特征的未来预测
   大样本的未来预测

   - 灰色预测模型（必须掌握）
     满足两个条件可用:
     a数据样本点个数少，6-15个
     b数据呈现指数或曲线的形式
   - 微分方程预测（备用）
     无法直接找到原始数据之间的关系，但可以找到原始数据变化速度之间
     的关系，通过公式推导转化为原始数据之间的关系。
   - 回归分析预测（必须掌握）
     如何变化;
     样本点的个数有要求:
     b样本点的个数n>3k+1，k为自变量的个数;
     c因变量要符合正态分布
     求一个因变量与若干自变量之间的关系，若自变量变化之后，求因变量
     a自变量之间协方差比较小，最好趋于零，自变量间的关系小;
   - 马尔科夫预测（备用）
     一个序列之间没有信息的传递，前后没有联系，数据与数据之间随机性强，相互不影响;今天的温度与昨天、后天没有直接联系，预测后天温度高、中、低的概率，只能得到概率
   - 时间序列预测（必须掌握)
     与马尔科夫预测互补，至少有2个点需要信息的传递，ARMA模型，周期模型，季节模型等。
   - 小波分析预测
   - 神经网络预测
   - 混沌序列预测

6. 优化与控制

   ①线性规划、整数规划、O-1规划（有约束，确定的目标）

   ②非线性规划与智能优化算法
   ④动态规划
   ⑤图论、网络优化（多因素交错复杂）
   ⑥排队论与计算机仿真
   ⑦模糊规划（范围约束)
   ⑧灰色规划（难）

   ③多目标规划和目标规划（柔性约束，目标含糊，超过)

## 模型规划

数学规划是运筹学的一个分支，其用来研究：在给定条件下（约束条件），如何按照某一衡量指标（目标函数）来寻求计划、管理工作中的最优方案。

#### 一般形式

- min或max   Z=f(x)     x:决策模型     f(x):目标函数、

- 一些不等式约束

  #### 数学规划的分类

  1. 线性规划如果目标函数f(x)和约束条件均是决策变量的线性达式，那么此时的数学规划问题就属于线推规划。
     1947年，美国数学家丹齐格提出了求解线惶规划的单纯形法，奠定了这门学斜的基础。

  2. 非线性规划

     当目标函数f(x)或者约束条件中有一个是决策变量x的非线性表达式，那么此时的数学国华问题就属于非线性规划。
     解决非线性规划要比线性规划困难得多，目前没有通用算法，大多数算法都是在选定决策变量的初始值后，通过一定的搜索方法寻求最右的决策变量

  3. 整数规划
     整数规划是一类要求变量取整数值的数学规划（线性整数规划、非线性整数规划）
     目前所流行的求解整数规划的算法瓦那个网只适用于线性整数规划

  4. 0-1规划

  ##### 求解线性规划

  ![image-20210311214219170](C:\Users\20844\AppData\Roaming\Typora\typora-user-images\image-20210311214219170.png)

  ![image-20210311213650691](C:\Users\20844\AppData\Roaming\Typora\typora-user-images\image-20210311213650691.png)

## 粒子群算法

 粒子群算法(particle swarm optimization，PSO)由Kennedy和Eberhart在1995年提出，该算法对于Hepper的模拟鸟群(鱼群)的模型进行修正，以使粒子能够飞向解空间，并在最好解处降落，从而得到了粒子群优化算法。同遗传算法类似，也是一种基于群体叠代的，但并没有遗传算法用的交叉以及变异，而是粒子在解空间追随最优的粒子进行搜索

 PSO的优势在于简单，容易实现，无需梯度信息，参数少，特别是其天然的实数编码特点特别适合于处理实优化问题。同时又有深刻的智能背景，既适合科学研究，又特别适合工程应用。
 设想这样一个场景：一群鸟在随机的搜索食物。在这个区域里只有一块食物，所有的鸟都不知道食物在哪。但是它们知道自己当前的位置距离食物还有多远。那么找到食物的最优策略是什么？最简单有效的就是搜寻目前离食物最近的鸟的周围区域。算法流程
参数定义每个寻优的问题解都被想像成一只鸟，称为“粒子”。所有粒子都在一个d维空间进行搜索。所有的粒子都由一个fitness-function确定适应值以判断目前的位置好坏。每一个粒子必须赋予记忆功能，能记住所搜寻到的最佳位置。每一个粒子还有一个速度以决定飞行

的距离和方向。这个速度根据它本身的飞行经验以及同伴的飞行经验进行动态调整。

 

在d维空间中，有m个粒子，在某一时刻时，

粒子i的位置为：

$$
x_i=(x_{i1},x_{i2},...,x_{id}),i=1,2,...,m
$$
粒子i的速度为：

$$
v_i = (v_{i1},v_{i2},...,v_{id}),i=1,2,...,m
$$
粒子i经过的历史最好位置：

$$
p_i = (p_{i1},p_{i2},...,p_{id}),i=1,2,...,m
$$
种群所经过的历史最好位置：

$$
p_g = (p_{g1},p_{g2},...,p_{gd})
$$
PSO的关系公式

鸟在捕食的过程中会根据自己的经验以及鸟群中的其他鸟的位置决定自己的速度，根据当前的位置和速度，可以得到下一刻的位置，这样每只鸟通过向自己和鸟群学习不断的更新自己的速度位置，最终找到食物，或者离食物足够近的点。

 

t时刻到t+1时刻的速度：

$$
v^{t+1} = wv^t_i + c_1r_1(p^t_i-x^t_i) + c_2r_2(p^t_g-x^t_i)
$$
下一时刻位置：(一般认为 v 后面还有个 t 但一般当做1来算)

$$
x^{t+1}_i = x^t_i + v^t_i
$$




![img](https://img-blog.csdn.net/20180121172938920?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lueWNn/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![img](https://img-blog.csdn.net/20180121174422234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvd2lueWNn/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

种群产生：随机产生处在[-10, 10]范围内的随机点，速度初始的为[0,1]

 在本例中，适应度就是函数值，适应度越小越好。在粒子群算法中，适应度不一定要越大越好，而是确定适应度的好坏，只需要根据是适应度好坏确定最佳位置。

 在迭代过程中，会有粒子跑出范围，在这种情况下，一般不强行将粒子重新拉回到初始化解空间。因为即使粒子跑出空间，随着迭代的进行，如果在初始化空间内有更好的解存在，那么粒子也可以自行返回到初始化空间。研究表明，即使将初始化空间不设为问题的约束空间，粒子也可能找到最优解

 

以求解函数最小值为例：
$$
f(x_1,x_2) = x_1^2 + x_2^2,x_1,x_2\epsilon
$$


```python
import numpy as np
import matplotlib.pyplot as plt

class PSO(object):
    def __init__(self, population_size, max_steps):
        self.w = 0.6  # 惯性权重
        self.c1 = self.c2 = 2
        self.population_size = population_size  # 粒子群数量
        self.dim = 2  # 搜索空间的维度
        self.max_steps = max_steps  # 迭代次数
        self.x_bound = [-10, 10]  # 解空间范围
        
        self.x = np.random.uniform(self.x_bound[0], self.x_bound[1],
                                   (self.population_size, self.dim))  # 初始化粒子群位置
        
        self.v = np.random.rand(self.population_size, self.dim)  # 初始化粒子群速度
        fitness = self.calculate_fitness(self.x)
        self.p = self.x  # 个体的最佳位置
        self.pg = self.x[np.argmin(fitness)]  # 全局最佳位置
        self.individual_best_fitness = fitness  # 个体的最优适应度
        self.global_best_fitness = np.min(fitness)  # 全局最佳适应度
        
        
        
def calculate_fitness(self, x):
    return np.sum(np.square(x), axis=1)
 
    
def evolve(self):
    fig = plt.figure()
    for step in range(self.max_steps):
        r1 = np.random.rand(self.population_size, self.dim)
        r2 = np.random.rand(self.population_size, self.dim)
        # 更新速度和权重
        self.v = self.w*self.v+self.c1*r1*(self.p-self.x)+self.c2*r2*(self.pg-self.x)
        self.x = self.v + self.x
        plt.clf()
        plt.scatter(self.x[:, 0], self.x[:, 1], s=30, color='k')
        plt.xlim(self.x_bound[0], self.x_bound[1])
        plt.ylim(self.x_bound[0], self.x_bound[1])
        plt.pause(0.01)
        fitness = self.calculate_fitness(self.x)
        # 需要更新的个体
        update_id = np.greater(self.individual_best_fitness, fitness)
        self.p[update_id] = self.x[update_id]
        self.individual_best_fitness[update_id] = fitness[update_id]
        # 新一代出现了更小的fitness，所以更新全局最优fitness和位置
        if np.min(fitness) < self.global_best_fitness:
            self.pg = self.x[np.argmin(fitness)]
            self.global_best_fitness = np.min(fitness)
        print('best fitness: %.5f, mean fitness: %.5f' % (self.global_best_fitness, np.mean(fitness)))
        
pso = PSO(100, 100)     
pso.evolve()
plt.show()
```

 





